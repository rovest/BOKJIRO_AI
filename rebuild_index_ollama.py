#!/usr/bin/env python3
"""
Ollama nomic-embed-text ì„ë² ë”©ìœ¼ë¡œ FAISS ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""
import json
import logging
from pathlib import Path
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from app.ollama_embeddings import get_ollama_embeddings

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_json_data(data_path: Path):
    """JSON ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
    logging.info(f"JSON ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def create_documents_from_json(json_data, max_docs=None):
    """JSON ë°ì´í„°ë¥¼ LangChain Document ê°ì²´ë¡œ ë³€í™˜ (í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìˆ˜ ì œí•œ ê°€ëŠ¥)"""
    documents = []
    data_to_process = json_data[:max_docs] if max_docs else json_data
    logging.info(f"ì´ {len(data_to_process)}ê°œ í•­ëª©ì„ Document ê°ì²´ë¡œ ë³€í™˜ ì¤‘...")

    for i, item in enumerate(data_to_process):
        if i % 1000 == 0:
            logging.info(f"ì§„í–‰ë¥ : {i}/{len(data_to_process)} ({i/len(data_to_process)*100:.1f}%)")

        doc = Document(
            page_content=item.get('text', ''),
            metadata=item.get('metadata', {})
        )
        documents.append(doc)

    logging.info(f"ì´ {len(documents)}ê°œ Document ìƒì„± ì™„ë£Œ")
    return documents

def build_faiss_index(documents, embeddings, output_path: Path):
    """FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥"""
    logging.info("FAISS ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")

    # ì‘ì€ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ì—¬ ì•ˆì •ì„± í™•ë³´
    batch_size = 100
    vector_db = None

    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        logging.info(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ë¬¸ì„œ)")

        try:
            if vector_db is None:
                vector_db = FAISS.from_documents(batch, embeddings)
            else:
                batch_db = FAISS.from_documents(batch, embeddings)
                vector_db.merge_from(batch_db)
        except Exception as e:
            logging.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # ì¸ë±ìŠ¤ ì €ì¥
    logging.info(f"FAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•©ë‹ˆë‹¤: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    vector_db.save_local(str(output_path))

    logging.info("FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ì €ì¥ ì™„ë£Œ!")
    return vector_db

def main():
    # ê²½ë¡œ ì„¤ì •
    base_dir = Path(__file__).parent
    data_file = base_dir / "data" / "vd_base_v2_refined.json"
    output_dir = base_dir / "db" / "faiss_index"

    if not data_file.exists():
        logging.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        return

    try:
        # Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logging.info("Ollama nomic-embed-text ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        embeddings = get_ollama_embeddings()
        logging.info("Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì²˜ìŒ 5000ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬
        logging.info("âš ï¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ì²˜ìŒ 5000ê°œ ë¬¸ì„œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë°ì´í„° ë¡œë“œ
        json_data = load_json_data(data_file)

        # Document ê°ì²´ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5000ê°œ ì œí•œ)
        documents = create_documents_from_json(json_data, max_docs=5000)

        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        build_faiss_index(documents, embeddings, output_dir)

        logging.info("âœ… Ollama ê¸°ë°˜ FAISS ì¸ë±ìŠ¤ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logging.info(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")
        logging.info("ğŸ”„ ì „ì²´ ë°ì´í„° ì²˜ë¦¬ë¥¼ ì›í•˜ì‹œë©´ max_docs=Noneìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")

    except Exception as e:
        logging.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()