# app/db_service.py

import numpy as np
import logging
from pathlib import Path
from typing import List, Dict, Optional
from collections import defaultdict, OrderedDict

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
# ë¡œì»¬ ì„ë² ë”©ì€ Streamlit Cloud ë°°í¬ ì‹œ ì œì™¸
# from .local_embeddings import get_local_embeddings  # BGE-M3 í™œì„±í™”
# from .ollama_embeddings import get_ollama_embeddings


# --- ìƒìˆ˜ ì •ì˜ (ë°°í¬ í™˜ê²½ í˜¸í™˜) ---
def get_faiss_path() -> str:
    """ë°°í¬ í™˜ê²½ì— ë§ëŠ” FAISS ê²½ë¡œë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •"""
    import os

    # í™˜ê²½ë³€ìˆ˜ë¡œ ê²½ë¡œ ì„¤ì • ê°€ëŠ¥
    if os.getenv('FAISS_PATH'):
        return os.getenv('FAISS_PATH')

    # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
    current_dir = Path(__file__).parent
    base_dir = current_dir.parent
    db_dir = base_dir / "db"

    # ìš°ì„ ìˆœìœ„: Google -> BGE -> Ollama
    possible_paths = [
        db_dir / "faiss_index_google_backup",
        db_dir / "faiss_index_bge",
        db_dir / "faiss_index"
    ]

    for path in possible_paths:
        if path.exists():
            return str(path)

    # ê¸°ë³¸ê°’ (ì—†ìœ¼ë©´ ëŸ°íƒ€ì„ì— ì—ëŸ¬ ë°œìƒ)
    return str(db_dir / "faiss_index_bge")

FAISS_PATH = get_faiss_path()


class DBService:
    """
    FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ë©°,
    ëª©ì°¨ ê¸°ë°˜ ê²€ìƒ‰ì„ í•µì‹¬ ì „ëµìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
    """
    def __init__(self, faiss_path=None, embedding_type="google"):
        logging.info("DEBUG: DBService ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")

        # ë°°í¬ í™˜ê²½ì—ì„œëŠ” FAISS ê²½ë¡œ ë™ì  ì„¤ì •
        if faiss_path is None:
            faiss_path = get_faiss_path()

        try:
            # Streamlit Cloud ë°°í¬ ì‹œì—ëŠ” Google ì„ë² ë”©ë§Œ ì‚¬ìš©
            if embedding_type == "google":
                self.embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
                logging.info("DEBUG: Google ìµœì‹  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì„±ê³µ.")
            else:
                # ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œë§Œ ë‹¤ë¥¸ ì„ë² ë”© ì‚¬ìš© ê°€ëŠ¥
                logging.warning(f"'{embedding_type}' ì„ë² ë”©ì€ ë°°í¬ í™˜ê²½ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Google ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                self.embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
                logging.info("DEBUG: Google ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´ ì™„ë£Œ.")
        except Exception as e:
            logging.error(f"!!! ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise ConnectionError(
                f"Google Embedding ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\n"
                f"- 'GOOGLE_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€.\n"
                f"- Google Cloud í”„ë¡œì íŠ¸ì—ì„œ 'Generative Language API'ê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€.\n"
                f"ìƒì„¸ ì˜¤ë¥˜: {e}"
            )

        try:
            absolute_faiss_path = str(Path(faiss_path).resolve())
            
            # FAISS ë²¡í„° DB ë¡œë“œ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
            self.vector_db = FAISS.load_local(
                absolute_faiss_path, self.embeddings, allow_dangerous_deserialization=True
            )
            
            self.all_docs = [self.vector_db.docstore.search(doc_id) 
                             for doc_id in self.vector_db.index_to_docstore_id.values() 
                             if self.vector_db.docstore.search(doc_id) is not None]
            
            # --- âœ¨ [í•µì‹¬ ë³µì›] ëª©ì°¨ ê²€ìƒ‰ì„ ìœ„í•œ ë³„ë„ DB ìƒì„± ---
            self.toc_docs = [
                doc for doc in self.all_docs 
                if doc.metadata.get('ì¤‘ë¶„ë¥˜') == 'ëª©ì°¨' and doc.metadata.get('í•­ëª©') == 'ì„¸ë¶€ëª©ì°¨'
            ]
            # ëª©ì°¨ ì „ìš© DB ìƒì„± (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
            self.toc_db = FAISS.from_documents(self.toc_docs, self.embeddings) if self.toc_docs else None
            logging.info(f"DEBUG: FAISS ë²¡í„° DB ë¡œë“œ ì„±ê³µ. ì „ì²´ {len(self.all_docs)}ê°œ ë¬¸ì„œ, ëª©ì°¨ {len(self.toc_docs)}ê°œ í•­ëª©.")
            
        except Exception as e:
            logging.error(f"!!! FAISS ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise FileNotFoundError(
                f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {faiss_path}\n"
                f"FAISS íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì„ë² ë”© ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
                f"ìƒì„¸ ì˜¤ë¥˜: {e}"
            )

    def get_schema_context(self) -> Dict[str, any]:
        """
        [âœ¨ ê°œì„ ì•ˆ] LLMì˜ ê²€ìƒ‰ ì„¤ê³„ë¥¼ ë•ê¸° ìœ„í•´ 'ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜' ì „ì²´ ê³„ì¸µ êµ¬ì¡°ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
        logging.debug("DEBUG: DBì˜ ì „ì²´ 'ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜' ê³„ì¸µ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        if not self.all_docs:
            return {'context_string': '', 'service_names': []}
        
        all_meta = [doc.metadata for doc in self.all_docs if doc and doc.metadata]
        
        # ì „ì²´ ì‚¬ì—…ëª… ëª©ë¡ ì¶”ì¶œ
        service_names = sorted(list(set(m.get('ì‚¬ì—…ëª…') for m in all_meta if m.get('ì‚¬ì—…ëª…'))))
        
        category_hierarchy = OrderedDict()

        for meta in all_meta:
            major_cat = meta.get('ëŒ€ë¶„ë¥˜')
            minor_cat = meta.get('ì¤‘ë¶„ë¥˜')
            
            if major_cat and minor_cat:
                if major_cat not in category_hierarchy:
                    category_hierarchy[major_cat] = set()
                category_hierarchy[major_cat].add(minor_cat)

        context_parts = ["# [ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡]"]
        for major, minors in category_hierarchy.items():
            context_parts.append(f"## {major}")
            for minor in sorted(list(minors)):
                context_parts.append(f"- {minor}")
            context_parts.append("")
        
        context_string = "\n".join(context_parts)
        
        logging.debug("LLMì— ì „ë‹¬ë  ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸:\n" + context_string)

        return {'context_string': context_string, 'service_names': service_names}
    
    def _search_by_metadata_filters(self, filters: Dict) -> List[Document]:
        """
        [ë‚´ë¶€ í—¬í¼] metadata_filtersì˜ ì—¬ëŸ¬ 'ì¤‘ë¶„ë¥˜' ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not filters or 'ì¤‘ë¶„ë¥˜' not in filters or not filters['ì¤‘ë¶„ë¥˜']:
            return self.all_docs

        target_categories = set(filters['ì¤‘ë¶„ë¥˜'])
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì‹œì‘ (ëŒ€ìƒ ì¤‘ë¶„ë¥˜: {target_categories})")
        
        matched_docs = [
            doc for doc in self.all_docs
            if doc.metadata.get('ì¤‘ë¶„ë¥˜') in target_categories
        ]
        
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²°ê³¼ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬.")
        return matched_docs

    def advanced_search(self, filters: Dict, keywords: List[str], k: int = 15) -> List[Document]:
        """
        [ìƒˆë¡œìš´ í•µì‹¬ ê²€ìƒ‰ í•¨ìˆ˜] ë©”íƒ€ë°ì´í„°ë¡œ 1ì°¨ í•„í„°ë§ í›„, í‚¤ì›Œë“œë¡œ 2ì°¨ ì •ë°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        logging.debug(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {filters}, í‚¤ì›Œë“œ: {keywords})")

        primary_docs = self._search_by_metadata_filters(filters)

        if not primary_docs:
            logging.warning("ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²°ê³¼, ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        logging.debug(f"ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ê°€ {len(primary_docs)}ê°œ ë¬¸ì„œë¡œ ì¢í˜€ì¡ŒìŠµë‹ˆë‹¤.")

        search_query = " ".join(keywords)
        
        # ì„ì‹œ DB ìƒì„± (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
        temp_db = FAISS.from_documents(primary_docs, self.embeddings)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
        final_docs = temp_db.similarity_search(query=search_query, k=k)


        logging.debug("\n" + "="*50)
        logging.debug(f"ğŸ•µï¸  [DB_SERVICE] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(final_docs)}ê°œ)")
        logging.debug(f"   - ì ìš©ëœ í•„í„°(ì¤‘ë¶„ë¥˜): {filters.get('ì¤‘ë¶„ë¥˜', 'N/A')}")
        logging.debug(f"   - ì ìš©ëœ í‚¤ì›Œë“œ: {keywords}")
        logging.debug("="*50)
        for i, doc in enumerate(final_docs):
            minor_category = doc.metadata.get('ì¤‘ë¶„ë¥˜', 'N/A')
            service_name = doc.metadata.get('ì‚¬ì—…ëª…', 'N/A')
            logging.debug(f"{i+1:02d}. ì¤‘ë¶„ë¥˜: {minor_category:<40} | ì‚¬ì—…ëª…: {service_name}")
        logging.debug("="*50 + "\n")

        return final_docs
    
    def metadata_search(self, filter_dict: Dict) -> List[Document]:
        """íŠ¹ì • ë©”íƒ€ë°ì´í„° ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {filter_dict})")
        matched_docs = []
        for doc in self.all_docs:
            if not (doc and doc.metadata): continue
            
            is_match = True
            for key, value in filter_dict.items():
                metadata_value = doc.metadata.get(key)
                if metadata_value is None or not str(metadata_value).startswith(str(value)):
                    is_match = False
                    break
            if is_match:
                matched_docs.append(doc)
        
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ê²°ê³¼ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬.")
        return matched_docs

    def __del__(self):
        pass
