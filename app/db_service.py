# app/db_service.py

import numpy as np
import logging
from pathlib import Path
from typing import List, Dict, Optional
from collections import defaultdict, OrderedDict

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from .local_embeddings import get_local_embeddings  # BGE-M3 í™œì„±í™”
from .ollama_embeddings import get_ollama_embeddings


# --- ìƒìˆ˜ ì •ì˜ ---
try:
    CURRENT_DIR = Path(__file__).parent
    BASE_DIR = CURRENT_DIR.parent
    DB_DIR = BASE_DIR / "db"
    # BGE-M3 ì „ìš© ì¸ë±ìŠ¤ ê²½ë¡œ
    FAISS_PATH = str(DB_DIR / "faiss_index_bge")
    if not (DB_DIR / "faiss_index_bge").exists():
        # ê¸°ì¡´ Ollama ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if (DB_DIR / "faiss_index").exists():
            FAISS_PATH = str(DB_DIR / "faiss_index")
        else:
            raise FileNotFoundError("FAISS index directory not found at any path.")
except NameError:
    BASE_DIR = Path.cwd()
    DB_DIR = BASE_DIR / "db"
    FAISS_PATH = str(DB_DIR / "faiss_index_bge")


class DBService:
    """
    FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ìƒí˜¸ì‘ìš©í•˜ë©°,
    ëª©ì°¨ ê¸°ë°˜ ê²€ìƒ‰ì„ í•µì‹¬ ì „ëµìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
    """
    def __init__(self, faiss_path=FAISS_PATH, embedding_type="ollama"):
        logging.info("DEBUG: DBService ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™” ì‹œì‘...")

        try:
            if embedding_type == "ollama":
                # Ollama ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (nomic-embed-text)
                logging.info("Ollama nomic-embed-text ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
                self.embeddings = get_ollama_embeddings()
                logging.info("DEBUG: Ollama ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì„±ê³µ.")
            elif embedding_type == "bge":
                # BGE-M3 ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
                logging.info("ë¡œì»¬ BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
                self.embeddings = get_local_embeddings()
                logging.info("DEBUG: BGE-M3 ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì„±ê³µ.")
            else:
                # Google ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© (ê¸°ì¡´ ë°©ì‹)
                self.embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
                logging.info("DEBUG: Google ìµœì‹  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì„±ê³µ.")
        except Exception as e:
            logging.error(f"!!! ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            if embedding_type == "ollama":
                raise ConnectionError(
                    f"Ollama ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
                    f"Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, nomic-embed-text ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                    f"ìƒì„¸ ì˜¤ë¥˜: {e}"
                )
            elif embedding_type == "bge":
                raise ConnectionError(
                    f"BGE-M3 ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\n"
                    f"ë‹¤ìŒ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”: sentence-transformers, FlagEmbedding\n"
                    f"ìƒì„¸ ì˜¤ë¥˜: {e}"
                )
            else:
                raise ConnectionError(
                    f"Google Embedding ëª¨ë¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:\n"
                    f"- 'GOOGLE_API_KEY' í™˜ê²½ ë³€ìˆ˜ê°€ .env íŒŒì¼ì— ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€.\n"
                    f"- Google Cloud í”„ë¡œì íŠ¸ì—ì„œ 'Generative Language API'ê°€ í™œì„±í™”ë˜ì—ˆëŠ”ì§€.\n"
                    f"ìƒì„¸ ì˜¤ë¥˜: {e}"
                )

        try:
            absolute_faiss_path = str(Path(faiss_path).resolve())
            
            # FAISS ë²¡í„° DB ë¡œë“œ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
            self.vector_db = FAISS.load_local(
                absolute_faiss_path, self.embeddings, allow_dangerous_deserialization=True
            )
            
            self.all_docs = [self.vector_db.docstore.search(doc_id) 
                             for doc_id in self.vector_db.index_to_docstore_id.values() 
                             if self.vector_db.docstore.search(doc_id) is not None]
            
            # --- âœ¨ [í•µì‹¬ ë³µì›] ëª©ì°¨ ê²€ìƒ‰ì„ ìœ„í•œ ë³„ë„ DB ìƒì„± ---
            self.toc_docs = [
                doc for doc in self.all_docs 
                if doc.metadata.get('ì¤‘ë¶„ë¥˜') == 'ëª©ì°¨' and doc.metadata.get('í•­ëª©') == 'ì„¸ë¶€ëª©ì°¨'
            ]
            # ëª©ì°¨ ì „ìš© DB ìƒì„± (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
            self.toc_db = FAISS.from_documents(self.toc_docs, self.embeddings) if self.toc_docs else None
            logging.info(f"DEBUG: FAISS ë²¡í„° DB ë¡œë“œ ì„±ê³µ. ì „ì²´ {len(self.all_docs)}ê°œ ë¬¸ì„œ, ëª©ì°¨ {len(self.toc_docs)}ê°œ í•­ëª©.")
            
        except Exception as e:
            logging.error(f"!!! FAISS ë²¡í„° DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise FileNotFoundError(
                f"FAISS ì¸ë±ìŠ¤ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {faiss_path}\n"
                f"FAISS íŒŒì¼ì´ ì¡´ì¬í•˜ê³  ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì„ë² ë”© ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n"
                f"ìƒì„¸ ì˜¤ë¥˜: {e}"
            )

    def get_schema_context(self) -> Dict[str, any]:
        """
        [âœ¨ ê°œì„ ì•ˆ] LLMì˜ ê²€ìƒ‰ ì„¤ê³„ë¥¼ ë•ê¸° ìœ„í•´ 'ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜' ì „ì²´ ê³„ì¸µ êµ¬ì¡°ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        """
        logging.debug("DEBUG: DBì˜ ì „ì²´ 'ëŒ€ë¶„ë¥˜-ì¤‘ë¶„ë¥˜' ê³„ì¸µ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        if not self.all_docs:
            return {'context_string': '', 'service_names': []}
        
        all_meta = [doc.metadata for doc in self.all_docs if doc and doc.metadata]
        
        # ì „ì²´ ì‚¬ì—…ëª… ëª©ë¡ ì¶”ì¶œ
        service_names = sorted(list(set(m.get('ì‚¬ì—…ëª…') for m in all_meta if m.get('ì‚¬ì—…ëª…'))))
        
        category_hierarchy = OrderedDict()

        for meta in all_meta:
            major_cat = meta.get('ëŒ€ë¶„ë¥˜')
            minor_cat = meta.get('ì¤‘ë¶„ë¥˜')
            
            if major_cat and minor_cat:
                if major_cat not in category_hierarchy:
                    category_hierarchy[major_cat] = set()
                category_hierarchy[major_cat].add(minor_cat)

        context_parts = ["# [ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡]"]
        for major, minors in category_hierarchy.items():
            context_parts.append(f"## {major}")
            for minor in sorted(list(minors)):
                context_parts.append(f"- {minor}")
            context_parts.append("")
        
        context_string = "\n".join(context_parts)
        
        logging.debug("LLMì— ì „ë‹¬ë  ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸:\n" + context_string)

        return {'context_string': context_string, 'service_names': service_names}
    
    def _search_by_metadata_filters(self, filters: Dict) -> List[Document]:
        """
        [ë‚´ë¶€ í—¬í¼] metadata_filtersì˜ ì—¬ëŸ¬ 'ì¤‘ë¶„ë¥˜' ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if not filters or 'ì¤‘ë¶„ë¥˜' not in filters or not filters['ì¤‘ë¶„ë¥˜']:
            return self.all_docs

        target_categories = set(filters['ì¤‘ë¶„ë¥˜'])
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì‹œì‘ (ëŒ€ìƒ ì¤‘ë¶„ë¥˜: {target_categories})")
        
        matched_docs = [
            doc for doc in self.all_docs
            if doc.metadata.get('ì¤‘ë¶„ë¥˜') in target_categories
        ]
        
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²°ê³¼ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬.")
        return matched_docs

    def advanced_search(self, filters: Dict, keywords: List[str], k: int = 15) -> List[Document]:
        """
        [ìƒˆë¡œìš´ í•µì‹¬ ê²€ìƒ‰ í•¨ìˆ˜] ë©”íƒ€ë°ì´í„°ë¡œ 1ì°¨ í•„í„°ë§ í›„, í‚¤ì›Œë“œë¡œ 2ì°¨ ì •ë°€ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        logging.debug(f"ê³ ê¸‰ ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {filters}, í‚¤ì›Œë“œ: {keywords})")

        primary_docs = self._search_by_metadata_filters(filters)

        if not primary_docs:
            logging.warning("ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê²°ê³¼, ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        logging.debug(f"ë©”íƒ€ë°ì´í„° í•„í„°ë§ìœ¼ë¡œ ê²€ìƒ‰ ë²”ìœ„ê°€ {len(primary_docs)}ê°œ ë¬¸ì„œë¡œ ì¢í˜€ì¡ŒìŠµë‹ˆë‹¤.")

        search_query = " ".join(keywords)
        
        # ì„ì‹œ DB ìƒì„± (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
        temp_db = FAISS.from_documents(primary_docs, self.embeddings)
        
        # ìœ ì‚¬ë„ ê²€ìƒ‰ (ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì•ˆì •ì„± ê°œì„ )
        final_docs = temp_db.similarity_search(query=search_query, k=k)


        logging.debug("\n" + "="*50)
        logging.debug(f"ğŸ•µï¸  [DB_SERVICE] ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ {len(final_docs)}ê°œ)")
        logging.debug(f"   - ì ìš©ëœ í•„í„°(ì¤‘ë¶„ë¥˜): {filters.get('ì¤‘ë¶„ë¥˜', 'N/A')}")
        logging.debug(f"   - ì ìš©ëœ í‚¤ì›Œë“œ: {keywords}")
        logging.debug("="*50)
        for i, doc in enumerate(final_docs):
            minor_category = doc.metadata.get('ì¤‘ë¶„ë¥˜', 'N/A')
            service_name = doc.metadata.get('ì‚¬ì—…ëª…', 'N/A')
            logging.debug(f"{i+1:02d}. ì¤‘ë¶„ë¥˜: {minor_category:<40} | ì‚¬ì—…ëª…: {service_name}")
        logging.debug("="*50 + "\n")

        return final_docs
    
    def metadata_search(self, filter_dict: Dict) -> List[Document]:
        """íŠ¹ì • ë©”íƒ€ë°ì´í„° ì¡°ê±´ê³¼ ì¼ì¹˜í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {filter_dict})")
        matched_docs = []
        for doc in self.all_docs:
            if not (doc and doc.metadata): continue
            
            is_match = True
            for key, value in filter_dict.items():
                metadata_value = doc.metadata.get(key)
                if metadata_value is None or not str(metadata_value).startswith(str(value)):
                    is_match = False
                    break
            if is_match:
                matched_docs.append(doc)
        
        logging.debug(f"DEBUG: ë©”íƒ€ë°ì´í„° ê²€ìƒ‰ ê²°ê³¼ {len(matched_docs)}ê°œ ë¬¸ì„œ ë°œê²¬.")
        return matched_docs

    def __del__(self):
        pass
